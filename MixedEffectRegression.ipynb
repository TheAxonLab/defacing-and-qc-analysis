{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0eb9d3f",
   "metadata": {},
   "source": [
    "# Defacing pre-registration - Statistical analysis in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7b55d",
   "metadata": {},
   "source": [
    "## Load simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb7b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- read.table(file = 'SimulatedDefacedRatings.csv', sep = \",\", header = TRUE, stringsAsFactors = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83f3e9",
   "metadata": {},
   "source": [
    "The simulated data were generated by running the `SimulateDefacedRatings.ipnyb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d73ec",
   "metadata": {},
   "source": [
    "## Continuation ratio mixed effects regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2a5ff",
   "metadata": {},
   "source": [
    "The ratings are ordinal, i.e. a categorical variable with a natural ordering of its levels. In order to model raters’ variabilities, we will use mixed effects regression from the GLMMadaptive package in R . This analysis is inspired from https://drizopoulos.github.io/GLMMadaptive/articles/Ordinal_Mixed_Models.html.\n",
    "\n",
    "The backward formulation is commonly used when progression through ratings from excluded, poor, good, excellent, is represented by increasing integer values, and interest lies in estimating the odds of better quality compared to worst quality. The forward formulation specifies that subjects have to ‘pass through’ one category to get to the next one. The backward formulation is thus suitable for our analysis.\n",
    "\n",
    "An advantage of the continuation ratio model is that its likelihood can be easily re-expressed such that it can be fitted with software the fits (mixed effects) logistic regression. This formulation requires a couple of data management steps creating separate records for each measurement, and suitably replicating the corresponding rows of the design matrices Xi and Zi. In addition, a new ‘cohort’ variable is constructed denoting at which category the specific measurement of i-th subject belongs. An extra advantage of this formulation is that we can easily evaluate if specific covariates satisfy the ordinality assumption (i.e., that their coefficients are independent of the category k) by including into the model their interaction with the ‘cohort’ variable and testing its significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfdebade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "mixed_model(fixed = ratings_new ~ cohort + defaced + rater, random = ~1 | \n",
       "    rater, data = cr_data, family = binomial(), na.action = na.exclude)\n",
       "\n",
       "\n",
       "Model:\n",
       " family: binomial\n",
       " link: logit \n",
       "\n",
       "Random effects covariance matrix:\n",
       "                 StdDev\n",
       "(Intercept) 0.009017118\n",
       "\n",
       "Fixed effects:\n",
       "      (Intercept)     cohorty<=good cohorty<=excluded   defacedoriginal \n",
       "    -0.9129735766      0.3668093138      0.6392333953     -0.0006134864 \n",
       "           rater2            rater3            rater4            rater5 \n",
       "    -0.1263234751     -0.0688977084     -0.1184201225      0.0001946318 \n",
       "           rater6            rater7            rater8            rater9 \n",
       "    -0.0242123897     -0.2368901397     -0.3441942007     -0.3221685507 \n",
       "          rater10           rater11           rater12 \n",
       "    -0.3926695910     -0.1975251922     -0.2950198344 \n",
       "\n",
       "log-Lik: -19062.91\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(GLMMadaptive)\n",
    "\n",
    "## Data set-up and calculation of marginal probabilites from a continuation ratio model\n",
    "cr_vals <- cr_setup(df$ratings, direction = \"backward\")\n",
    "cr_data <- df[cr_vals$subs, ]\n",
    "cr_data$ratings_new <- cr_vals$y\n",
    "cr_data$cohort <- cr_vals$cohort\n",
    "\n",
    "## Fits generalized linear mixed effects model\n",
    "fm <- mixed_model(ratings_new ~ cohort + defaced + rater, random = ~ 1 | rater, data = cr_data, family = binomial(), na.action = na.exclude)\n",
    "fm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20624af8",
   "metadata": {},
   "source": [
    "`random = ~ 1 | rater` enables to account for between-raters variability. This formula lets the intercept take a different value for each rater."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb978a65",
   "metadata": {},
   "source": [
    "### Relaxing CR assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e0912",
   "metadata": {},
   "source": [
    "We can relax the ordinality assumption for the defaced variable, namely, allowing that the effect of defaced is different for each of the response categories of our ordinal outcome y. This can be achieved by simply including the interaction term between the defaced and cohort variables and/or the interaction term between the rater and the cohort variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9bb02ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "mixed_model(fixed = ratings_new ~ cohort * defaced + rater, random = ~1 | \n",
       "    rater, data = cr_data, family = binomial())\n",
       "\n",
       "\n",
       "Model:\n",
       " family: binomial\n",
       " link: logit \n",
       "\n",
       "Random effects covariance matrix:\n",
       "                StdDev\n",
       "(Intercept) 0.00835499\n",
       "\n",
       "Fixed effects:\n",
       "                 (Intercept)                cohorty<=good \n",
       "                 -1.27016209                   0.37607078 \n",
       "               cohorty<=poor               defaceddefaced \n",
       "                  1.10847654                   0.54198717 \n",
       "                      rater2                       rater3 \n",
       "                 -0.06513242                  -0.05340367 \n",
       "                      rater4                       rater5 \n",
       "                  0.02817786                   0.17916487 \n",
       "                      rater6                       rater7 \n",
       "                  0.11767470                   0.14844202 \n",
       "                      rater8                       rater9 \n",
       "                  0.25101273                   0.19715950 \n",
       "                     rater10                      rater11 \n",
       "                  0.25054147                   0.46844531 \n",
       "                     rater12 cohorty<=good:defaceddefaced \n",
       "                  0.48464015                  -0.27011929 \n",
       "cohorty<=poor:defaceddefaced \n",
       "                  0.08436671 \n",
       "\n",
       "log-Lik: -18837.41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gm <- mixed_model(ratings_new ~ cohort * defaced + rater, random = ~ 1 | rater, data = cr_data, family = binomial())\n",
    "gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89bb2841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "mixed_model(fixed = ratings_new ~ cohort * defaced + rater, random = ~1 | \n",
       "    rater, data = cr_data, family = binomial())\n",
       "\n",
       "\n",
       "Model:\n",
       " family: binomial\n",
       " link: logit \n",
       "\n",
       "Random effects covariance matrix:\n",
       "                 StdDev\n",
       "(Intercept) 0.008354982\n",
       "\n",
       "Fixed effects:\n",
       "                  (Intercept)                 cohorty<=good \n",
       "                  -0.72818126                    0.10594841 \n",
       "                cohorty<=poor               defacedoriginal \n",
       "                   1.19284550                   -0.54196754 \n",
       "                       rater2                        rater3 \n",
       "                  -0.06513242                   -0.05340367 \n",
       "                       rater4                        rater5 \n",
       "                   0.02817786                    0.17916487 \n",
       "                       rater6                        rater7 \n",
       "                   0.11767470                    0.14844202 \n",
       "                       rater8                        rater9 \n",
       "                   0.25101273                    0.19715950 \n",
       "                      rater10                       rater11 \n",
       "                   0.25054147                    0.46844531 \n",
       "                      rater12 cohorty<=good:defacedoriginal \n",
       "                   0.48464015                    0.27012740 \n",
       "cohorty<=poor:defacedoriginal \n",
       "                  -0.08436897 \n",
       "\n",
       "log-Lik: -18837.41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gm2 <- mixed_model(ratings_new ~ cohort * defaced + cohort * rater, random = ~ 1 | rater, data = cr_data, family = binomial())\n",
    "#gm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad207d3",
   "metadata": {},
   "source": [
    "To test whether these extensions are required we can perform a likelihood ratio test using the anova() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "957f753c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        AIC      BIC   log.Lik   LRT df p.value\n",
       "fm 37740.76 37748.52 -18854.38                 \n",
       "gm 37710.82 37719.54 -18837.41 33.94  2 <0.0001\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anova(fm,gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2fdeca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "         AIC      BIC   log.Lik   LRT df p.value\n",
       "gm  37710.82 37719.54 -18837.41                 \n",
       "gm2 37730.15 37749.54 -18825.07 24.67 22  0.3132\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#anova(gm, gm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d7253",
   "metadata": {},
   "source": [
    "**Discussion** The first test rejects the null hypothesis meaning that defaced doesn't satisfy the continuous ratio assumption => the addition of the interaction term cohort x defaced is thus necessary. However, the second test doesn't reject the null hypothesis, meaning that rater satisfied the continuation ratio assumption, thus the interaction term cohort x rater is not necessary.\n",
    "\n",
    "gm is our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89a5ff",
   "metadata": {},
   "source": [
    "### Effect plot of conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1f4358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in model.frame.default(termsX, newdata, xlev = .getXlevels(termsX, :\n",
      "“variable 'rater' is not a factor”\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in X %*% betas: non-conformable arguments\n",
     "output_type": "error",
     "traceback": [
      "Error in X %*% betas: non-conformable arguments\nTraceback:\n",
      "1. effectPlotData(gm, nDF, direction = \"backward\")",
      "2. effectPlotData.MixMod(gm, nDF, direction = \"backward\")"
     ]
    }
   ],
   "source": [
    "library(lattice)\n",
    "\n",
    "#Extract data necessary to plot\n",
    "nDF <- with(cr_data, expand.grid(cohort = levels(cohort), defaced = levels(defaced), \n",
    "                                 rater = rater))\n",
    "\n",
    "plot_data <- effectPlotData(gm, nDF, direction=\"backward\")\n",
    "\n",
    "#Plot\n",
    "expit <- function (x) exp(x) / (1 + exp(x))\n",
    "my_panel_bands <- function(x, y, upper, lower, fill, col, subscripts, ..., font, \n",
    "                           fontface) {\n",
    "    upper <- upper[subscripts]\n",
    "    lower <- lower[subscripts]\n",
    "    panel.polygon(c(x, rev(x)), c(upper, rev(lower)), col = fill, border = FALSE, ...)\n",
    "}\n",
    "\n",
    "\n",
    "xyplot(expit(pred) ~ rater | defaced, group = cohort, data = plot_data, \n",
    "       upper = expit(plot_data$upp), low = expit(plot_data$low), type = \"l\",\n",
    "       panel = function (x, y, ...) {\n",
    "           panel.superpose(x, y, panel.groups = my_panel_bands, ...)\n",
    "           panel.xyplot(x, y, lwd = 2,  ...)\n",
    "       }, xlab = \"Rater\", ylab = \"Continuation Ratio Probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26acb4",
   "metadata": {},
   "source": [
    "### Effect plot of marginal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8b6acc",
   "metadata": {},
   "source": [
    "The effect plot of the previous section depicts the conditional probabilities according to the backward formulation of the continuation ratio model. However, it is easier to understand the marginal probabilities of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c01a4b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in model.frame.default(termsX, newdata, xlev = .getXlevels(termsX, :\n",
      "“variable 'rater' is not a factor”\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in X %*% betas: non-conformable arguments\n",
     "output_type": "error",
     "traceback": [
      "Error in X %*% betas: non-conformable arguments\nTraceback:\n",
      "1. effectPlotData(fm, nDF, CR_cohort_varname = \"cohort\", direction = \"backward\")",
      "2. effectPlotData.MixMod(fm, nDF, CR_cohort_varname = \"cohort\", \n .     direction = \"backward\")",
      "3. do.call(\"cbind\", split(c(X %*% betas), cohort_var))",
      "4. split(c(X %*% betas), cohort_var)"
     ]
    }
   ],
   "source": [
    "#Extract data for plot\n",
    "plot_data_m <- effectPlotData(fm, nDF, CR_cohort_varname = \"cohort\", \n",
    "                              direction = \"backward\")\n",
    "#Plot\n",
    "key <- list(space = \"top\", rep = FALSE,\n",
    "            text = list(levels(DF$y)[1:2]),\n",
    "            lines = list(lty = c(1, 1), lwd = c(2, 2), col = c(\"#0080ff\", \"#ff00ff\")),\n",
    "            text = list(levels(DF$y)[3:4]),\n",
    "            lines = list(lty = c(1, 1), lwd = c(2, 2), col = c(\"darkgreen\", \"#ff0000\")))\n",
    "\n",
    "xyplot(expit(pred) ~ time | sex, group = ordinal_response, data = plot_data_m, \n",
    "       upper = expit(plot_data_m$upp), low = expit(plot_data_m$low), type = \"l\",\n",
    "       panel = function (x, y, ...) {\n",
    "           panel.superpose(x, y, panel.groups = my_panel_bands, ...)\n",
    "           panel.xyplot(x, y, lwd = 2, ...)\n",
    "       }, xlab = \"Follow-up time\", ylab = \"Marginal Probabilities\", key = key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7e3b6",
   "metadata": {},
   "source": [
    "### Ordered logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d057d5",
   "metadata": {},
   "source": [
    "Test just ordered logistic regression before increasing the complexity using mixed_model. Inspired from https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e08f97c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘MASS’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:GLMMadaptive’:\n",
      "\n",
      "    negative.binomial\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "polr(formula = ratings ~ defaced + rater, data = df, Hess = TRUE, \n",
       "    method = \"logistic\")\n",
       "\n",
       "Coefficients:\n",
       "defacedoriginal           rater \n",
       "    -0.54861742      0.05227948 \n",
       "\n",
       "Intercepts:\n",
       " excluded|poor      poor|good good|excellent \n",
       "    -1.3730969     -0.1424015      0.9006451 \n",
       "\n",
       "Residual Deviance: 37781.85 \n",
       "AIC: 37791.85 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(MASS)\n",
    "m <- polr(ratings ~ defaced + rater, data = df, Hess=TRUE, method = \"logistic\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12351ac3",
   "metadata": {},
   "source": [
    "One way to calculate a p-value in this case is by comparing the t-value against the standard normal distribution, like a z test. Of course this is only true with infinite degrees of freedom, but is reasonably approximated by large samples, becoming increasingly biased as sample size decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80492dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 5 × 4 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Value</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>p value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>defacedoriginal</th><td>-0.54861742</td><td>0.03072127</td><td>-17.857904</td><td> 2.508829e-71</td></tr>\n",
       "\t<tr><th scope=row>rater</th><td> 0.05227948</td><td>0.00443805</td><td> 11.779831</td><td> 4.959221e-32</td></tr>\n",
       "\t<tr><th scope=row>excluded|poor</th><td>-1.37309692</td><td>0.03900182</td><td>-35.205974</td><td>1.620050e-271</td></tr>\n",
       "\t<tr><th scope=row>poor|good</th><td>-0.14240145</td><td>0.03691166</td><td> -3.857899</td><td> 1.143660e-04</td></tr>\n",
       "\t<tr><th scope=row>good|excellent</th><td> 0.90064511</td><td>0.03770962</td><td> 23.883699</td><td>4.524274e-126</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 5 × 4 of type dbl\n",
       "\\begin{tabular}{r|llll}\n",
       "  & Value & Std. Error & t value & p value\\\\\n",
       "\\hline\n",
       "\tdefacedoriginal & -0.54861742 & 0.03072127 & -17.857904 &  2.508829e-71\\\\\n",
       "\trater &  0.05227948 & 0.00443805 &  11.779831 &  4.959221e-32\\\\\n",
       "\texcluded\\textbar{}poor & -1.37309692 & 0.03900182 & -35.205974 & 1.620050e-271\\\\\n",
       "\tpoor\\textbar{}good & -0.14240145 & 0.03691166 &  -3.857899 &  1.143660e-04\\\\\n",
       "\tgood\\textbar{}excellent &  0.90064511 & 0.03770962 &  23.883699 & 4.524274e-126\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 5 × 4 of type dbl\n",
       "\n",
       "| <!--/--> | Value | Std. Error | t value | p value |\n",
       "|---|---|---|---|---|\n",
       "| defacedoriginal | -0.54861742 | 0.03072127 | -17.857904 |  2.508829e-71 |\n",
       "| rater |  0.05227948 | 0.00443805 |  11.779831 |  4.959221e-32 |\n",
       "| excluded|poor | -1.37309692 | 0.03900182 | -35.205974 | 1.620050e-271 |\n",
       "| poor|good | -0.14240145 | 0.03691166 |  -3.857899 |  1.143660e-04 |\n",
       "| good|excellent |  0.90064511 | 0.03770962 |  23.883699 | 4.524274e-126 |\n",
       "\n"
      ],
      "text/plain": [
       "                Value       Std. Error t value    p value      \n",
       "defacedoriginal -0.54861742 0.03072127 -17.857904  2.508829e-71\n",
       "rater            0.05227948 0.00443805  11.779831  4.959221e-32\n",
       "excluded|poor   -1.37309692 0.03900182 -35.205974 1.620050e-271\n",
       "poor|good       -0.14240145 0.03691166  -3.857899  1.143660e-04\n",
       "good|excellent   0.90064511 0.03770962  23.883699 4.524274e-126"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Store table\n",
    "ctable <- coef(summary(m))\n",
    "\n",
    "## calculate and store p values\n",
    "p <- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n",
    "\n",
    "## combined table\n",
    "(ctable <- cbind(ctable, \"p value\" = p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0004dd7",
   "metadata": {},
   "source": [
    "We can also get confidence intervals for the parameter estimates. These can be obtained either by profiling the likelihood function or by using the standard errors and assuming a normal distribution. Note that profiled CIs are not symmetric (although they are usually close to symmetric). If the 95% CI does not cross 0, the parameter estimate is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd45b70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for profiling to be done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 2 × 2 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>defacedoriginal</th><td>-0.60886781</td><td>-0.48844059</td></tr>\n",
       "\t<tr><th scope=row>rater</th><td> 0.04358562</td><td> 0.06098284</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2 × 2 of type dbl\n",
       "\\begin{tabular}{r|ll}\n",
       "  & 2.5 \\% & 97.5 \\%\\\\\n",
       "\\hline\n",
       "\tdefacedoriginal & -0.60886781 & -0.48844059\\\\\n",
       "\trater &  0.04358562 &  0.06098284\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2 × 2 of type dbl\n",
       "\n",
       "| <!--/--> | 2.5 % | 97.5 % |\n",
       "|---|---|---|\n",
       "| defacedoriginal | -0.60886781 | -0.48844059 |\n",
       "| rater |  0.04358562 |  0.06098284 |\n",
       "\n"
      ],
      "text/plain": [
       "                2.5 %       97.5 %     \n",
       "defacedoriginal -0.60886781 -0.48844059\n",
       "rater            0.04358562  0.06098284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 2 × 2 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>2.5 %</th><th scope=col>97.5 %</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>defacedoriginal</th><td>-0.60882999</td><td>-0.4884048</td></tr>\n",
       "\t<tr><th scope=row>rater</th><td> 0.04358106</td><td> 0.0609779</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2 × 2 of type dbl\n",
       "\\begin{tabular}{r|ll}\n",
       "  & 2.5 \\% & 97.5 \\%\\\\\n",
       "\\hline\n",
       "\tdefacedoriginal & -0.60882999 & -0.4884048\\\\\n",
       "\trater &  0.04358106 &  0.0609779\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2 × 2 of type dbl\n",
       "\n",
       "| <!--/--> | 2.5 % | 97.5 % |\n",
       "|---|---|---|\n",
       "| defacedoriginal | -0.60882999 | -0.4884048 |\n",
       "| rater |  0.04358106 |  0.0609779 |\n",
       "\n"
      ],
      "text/plain": [
       "                2.5 %       97.5 %    \n",
       "defacedoriginal -0.60882999 -0.4884048\n",
       "rater            0.04358106  0.0609779"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(ci <- confint(m)) # default method gives profiled CIs\n",
    "confint.default(m) # CIs assuming normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150178b6",
   "metadata": {},
   "source": [
    "Both the p-values and the confidence interval indicate that both the defacing and rater predictors improve the fit of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f48528",
   "metadata": {},
   "source": [
    "#### Verifying proportional odds assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3982f8",
   "metadata": {},
   "source": [
    "One of the assumptions underlying ordinal logistic (and ordinal probit) regression is that the relationship between each pair of outcome groups is the same. In other words, ordinal logistic regression assumes that the coefficients that describe the relationship between, say, the lowest versus all higher categories of the response variable are the same as those that describe the relationship between the next lowest category and all higher categories, etc. This is called the proportional odds assumption or the parallel regression assumption. Because the relationship between all pairs of groups is the same, there is only one set of coefficients. If this was not the case, we would need different sets of coefficients in the model to describe the relationship between each pair of outcome groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6cea9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Length   Class    Mode \n",
       "      3 formula    call "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The sf function will calculate the log odds of being greater than or equal to each value of the target variable\n",
    "sf <- function(y) {\n",
    "  c('Y>=1' = qlogis(mean(y >= 1)),\n",
    "    'Y>=2' = qlogis(mean(y >= 2)),\n",
    "    'Y>=3' = qlogis(mean(y >= 3)),\n",
    "    'Y>=4' = qlogis(mean(y >= 4)))\n",
    "}\n",
    "\n",
    "#calls the function sf on several subsets of the data defined by the predictors\n",
    "(s <- with(df, summary(as.numeric(ratings) ~ defaced + rater, fun=sf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdb831",
   "metadata": {},
   "source": [
    "The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. We can evaluate the parallel slopes assumption by running a series of binary logistic regressions with varying cutpoints on the dependent variable and checking the equality of coefficients across cutpoints. We thus relax the parallel slopes assumption to checks its tenability. To accomplish this, we transform the original, ordinal, dependent variable into a new, binary, dependent variable which is =0 if the original, ordinal dependent variable (here apply) is < some value a, and 1 if the ordinal variable is >= a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34385e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  glm(formula = I(as.numeric(ratings) >= 2) ~ defaced, family = \"binomial\", \n",
       "    data = df)\n",
       "\n",
       "Coefficients:\n",
       "    (Intercept)  defacedoriginal  \n",
       "         1.8206          -0.7326  \n",
       "\n",
       "Degrees of Freedom: 13919 Total (i.e. Null);  13918 Residual\n",
       "Null Deviance:\t    13760 \n",
       "Residual Deviance: 13480 \tAIC: 13480"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm(I(as.numeric(ratings) >= 2) ~ defaced, family=\"binomial\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2ef02a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  glm(formula = I(as.numeric(ratings) >= 3) ~ defaced, family = \"binomial\", \n",
       "    data = df)\n",
       "\n",
       "Coefficients:\n",
       "    (Intercept)  defacedoriginal  \n",
       "         0.4409          -0.4593  \n",
       "\n",
       "Degrees of Freedom: 13919 Total (i.e. Null);  13918 Residual\n",
       "Null Deviance:\t    19150 \n",
       "Residual Deviance: 18970 \tAIC: 18970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm(I(as.numeric(ratings) >= 3) ~ defaced, family=\"binomial\", data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d6c7d",
   "metadata": {},
   "source": [
    "When defaced = 'defaced', the difference between the predicted value for apply >=2 and apply >=3 is roughly 2 (1.82 + 0.44 = 2.26). For defaced = 'original', the difference in predicted values for apply >= 2 and apply >=3 is roughly 1 ((1.82-0.73)+(0.44-0.46) = 1.07). This suggests that the parallel slopes assumption is not reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da207b",
   "metadata": {},
   "source": [
    "### Test significance of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9404d",
   "metadata": {},
   "source": [
    "We test significance of the predictors using Wald test. Specifically, we want to test whether the coefficient associated to the fixed effect defacing is significantly non-zero. Note that the use of this test is possible because we have a balanced, nested GLMM. Inspired from https://www.statology.org/wald-test-in-r/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(aod)\n",
    "\n",
    "#perform Wald Test to determine if defacing variables is zero\n",
    "#This is copied from an example using lm but doesn't work for mixed_model\n",
    "wald.test(Sigma = vcov(fm), b = coef(fm), Terms = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8b4fe",
   "metadata": {},
   "source": [
    "If p-value > 0.05, it means we fail to reject the null hypothesis, meaning defacing coefficient is approximately zero. Thus you can drop that predictor from the model, because it doesn't significantly improve the fit of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
